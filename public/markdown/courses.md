# NEXT-PERN-COURSES ğŸ“š

- The name might seem confusing at first, but it's intentional
  - `NEXT` stands for `Next.js`, the frontend framework used.
  - `PERN` usually means `PostgreSQL`, `Express`, `React`, `Node` â€” but in my case, I swapped `Express` with `Fastify` to explore a new modern backend framework.

## ğŸ§° Tech Stack

### Frontend

- **Next.js** â€“ App directory routing, SSR/CSR mix
- **React** â€“ Core UI library
- **react-hook-form** â€“ Minimal and performant form state management
- **Redux Toolkit** â€“ Global state management
- **RTK Query** â€“ Data fetching and caching
- **Axios** â€“ Used for HTTP requests and configured as RTK Query's base query
- **Tailwind CSS** â€“ Utility-first styling
- **Framer Motion** â€“ Smooth animations
- **Emotion** â€“ CSS-in-JS for dynamic styles
- **Sass** â€“ Used for global styling and design tokens
- **react-markdown** â€“ Render markdown content in React
- **DOMPurify** â€“ Sanitizes markdown HTML output to prevent XSS attacks

### Backend

- **Fastify** â€“ High-performance Node.js web framework
- **Prisma** â€“ Type-safe ORM for PostgreSQL
- **PostgreSQL** â€“ Relational database
- **Cloudinary** â€“ Media hosting and delivery (used for uploading and storing course/concept images or videos)
- **sql-template-tag** â€“ Safe, readable raw SQL syntax for advanced custom queries outside Prisma's API

### Frontend && Backend

- **TypeScript** â€“ Full type safety across the stack
- **Zod** â€“ Runtime validation for inputs and schemas (schemas shared between client and server via the `packages/` folder)

### DevOps & Infrastructure

- **Docker** â€“ Containerized development and deployment
  - Backend and client run in isolated containers
- **GitHub Actions** â€“ CI/CD pipelines
  - Runs linting and type-checking on every push or merge to `main` (CI)
  - Deploys both client and server to Fly.io automatically on success (CD)
- **Nginx** â€“ Reverse proxy local development
- **Fly.io** â€“ Deployment platform for both frontend and backend (Docker-based)
- **Vercel** â€” Initially used for client deployment, replaced by Fly.io
- **Supabase** â€“ Hosted PostgreSQL database provider

### Tooling

- **Turborepo** â€“ Monorepo management
- **Yarn Berry (with node_modules linker)** â€“ Package manager

## ğŸ› ï¸ Setup

- **Database Hosting** â€“ For this project, I used [Supabase](https://supabase.com) because my free-tier on [Aiven](https://aiven.io) didnâ€™t allow creating new databases.  
  You can use either [Aiven](https://aiven.io) or [Supabase](https://supabase.com) â€” both are excellent platforms for hosting PostgreSQL databases.

- **App Hosting Platforms** â€“ So far Iâ€™ve used [Render](https://render.com),[Fly.io](https://fly.io) and [Vercel](https://vercel.com) for deployment.
  - **Render** offers a completely free plan, but cold starts can sometimes be noticeably slow.
  - **Fly.io** starts with a $5 monthly credit for outbound data and traffic. After that, it operates on a pay-as-you-go model.
  - **Vercel** has no cold starts and offers the best integration for **Next.js apps**. However, it's only suitable for backends if you're using a **serverless** structure â€” traditional custom servers (like Fastify) aren't supported directly.

If you decide to use [Fly.io](https://fly.io), you'll need to:

1. Create an account and go to the **Access Tokens** section.
2. Generate **two tokens**: one for the client and one for the server (each has its own `Dockerfile` and `fly.toml`).
3. Add these tokens to your repo's settings under  
   **Settings â†’ Secrets and variables â†’ Actions â†’ Repository secrets**.

This allows GitHub Actions to authenticate and deploy both apps during CI/CD.

If you choose [Render](https://render.com) or [Vercel](https://vercel.com), deployments happen automatically on `push` or `merge` to the `main` branch â€” no custom CI/CD pipeline is required.

### âš ï¸ Deployment Considerations: Split Client & Server Hosting

Hosting the frontend (`Next.js`) and backend (`Fastify API`) on **different platforms** (e.g. `Vercel` + `Fly.io` or `Render`) can introduce deployment issues that are important to keep in mind:

- **Race conditions during deployment**  
  If the `Next.js` app (hosted on `Vercel` or `Fly.io`) builds and runs before the Fastify server is fully deployed, any API calls made at build time can fail resulting in 500 errors.

- **Cold start sync issues**  
  When the frontend is live on `Vercel` but the backend is still waking up (e.g. on `Render` or `Fly.io`), youâ€™ll experience `500 Internal Server Errors` on the frontend until the server becomes responsive.  
  This requires **manual delay handling or retry strategies**.

ğŸ§  For this reason, I moved both frontend and backend to **Fly.io**, using Docker for consistent deploy timing.

## ğŸ§¬ Database Models

For this project, I chose to explore a new modern ORM â€” [Prisma](https://www.prisma.io/) â€” instead of using [Sequelize](https://sequelize.org/), which I had already worked with in the past

Prisma uses its own **domain-specific language (DSL)** for modeling data, and all models are typically managed in a single `schema.prisma` file located in the `prisma/` directory.

While Prisma is great for type safety and migrations, it doesn't natively support splitting models across multiple files â€” which can lead to a bloated `schema.prisma` as your app grows.

To work around this, I created a custom solution:

- I keep individual model files in a separate `models/` directory
- I use a **Bash script** to merge them in order into a single `schema.prisma`
- Then I trigger the Prisma migration with a custom name

### ğŸ› ï¸ Schema Merge Script

```bash
# dmm stands for database merge and migrate
dmm() {
  local dev_dir="./models"
  local prisma_dir="./prisma"
  local output_file="schema.prisma"
  local output_path="$prisma_dir/$output_file"

  # clear all previous content from schema.prisma
  : > "$output_path"

  for f in "$dev_dir"/*; do
  # skip if current item is not a file
    [[ ! -f "$f" ]] && continue
  # Add clear boundaries for each file's contents
    echo -e "\n/* => start $(basename "$f") */" >> "$output_path"
    cat "$f" >> "$output_path"
    echo -e "\n/* => end $(basename "$f") */" >> "$output_path"
  done

  echo "Merged schemas to $output_path"
  yarn prisma migrate dev --name "$1"
}
```

## ğŸ›¡ï¸ Reverse Proxy (optional)

As part of my local development setup, I prefer to start an `NGINX` server so the environment closely mirrors the production setup.

This allows me to:

- Test CORS issues directly in development
- Simulate deployment flow more realistically

ğŸ’¡ **Note:** â€” Youâ€™ll need to configure Node.js to trust self-signed certs.

Add the following line to your `.bashrc`, `.zshrc`, or shell config:

```bash
export NODE_OPTIONS="--use-system-ca"
```

For Self-signed certs I used `mkcert` and `nss`(optional for `Chrome` â€” required just for `Firefox`).
Setup includes:

```bash
sudo pacman -S mkcert nss && \
mkcert -install && \
mkcert localhost
```

### ğŸ“œ NGINX Config Script

ğŸ’¡ Wherever you see `ninja` in paths (e.g. `/home/ninja/`), replace it with **your Linux username**, which you chose during OS installation.
You can check your current username with:

```bash
echo $USER
```

```bash
# http is the default NGINX user on Manjaro (Arch-based distros).
# It's preferred over 'root' to avoid permission issues
user http;
worker_processes auto;

events {
    worker_connections 1024;
}

http {
    # Load MIME types from file to recognize extensions
    include mime.types;

    # Default to raw binary if MIME type is unknown
    default_type application/octet-stream;

    # Let the kernel handle file transfers for performance
    sendfile on;

    # Keep connections open for 65s before timing out
    keepalive_timeout 65;

    # Hide the NGINX version (like Helmet does for Node)
    server_tokens off;

    # Allocate more memory for MIME type hash table
    types_hash_max_size 2048;
    types_hash_bucket_size 128;

    # HTTP â†’ HTTPS redirect
    server {
        listen 80;
        server_name localhost;

    # 301 code is a permanent redirection
        location / {
            return 301 https://$host$request_uri;
        }
    }

    # âœ… HTTPS server with reverse proxy
    server {
        listen 443 ssl;
        server_name localhost;

        # Increase body size limit (useful for file uploads)
        client_max_body_size 200M;

        # Restrict to modern TLS versions only
        ssl_protocols TLSv1.2 TLSv1.3;
        # exclude ciphers that allow anonymous key exchange and weak hashing algorithms
        ssl_ciphers HIGH:!aNULL:!MD5;

        # Basic logging
        access_log /var/log/nginx/access.log;
        error_log  /var/log/nginx/error.log warn;

        # ğŸ” SSL cert paths (use absolute paths)
        ssl_certificate     /home/ninja/certs/nginx-dev/localhost.pem;
        ssl_certificate_key /home/ninja/certs/nginx-dev/localhost-key.pem;

        # ğŸ” Backend API (Fastify on :3000)
        location /api/v1/ {
            proxy_pass http://localhost:3000/api/v1/;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_cache_bypass $http_upgrade;
        }

        # ğŸŒ Frontend App (Next.js on :3001)
        location / {
            proxy_pass http://localhost:3001/;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_cache_bypass $http_upgrade;
        }
    }
}
```

## Branches ğŸŒ´

Even side branches were deleted after merge,they can still be recognized in Git log because I pass as option `--no-ff` to ensure a merge commit is always created.
I use a script to append the branch name I am working in to the start of commit message so it will be easy to recognize in future.

Result is something like:

```bash
sidebar_logic => created redux slice of sidebar
```

## App Logic âš™ï¸

### Typescript config ğŸŸ¦

The client and server share Zod schemas, TypeScript types, and utility functions via a common package:

- Located at: `packages/shared`
- Imported using a path alias: `@shared/first`

ğŸ“˜ Any `tsconfig.json` used as a reference in other packages **must include**:

```json
{
  "compilerOptions": {
    "composite": true
  }
}
```

his makes shared packages (like `packages/shared`) usable across the monorepo â€” allowing you to alias and import them cleanly

### ğŸ“‚ Folders Architecture

Both client and server follow a **feature-based architecture** that aims to encapsulate logic per domain while keeping shared utilities and reusable logic at the root level.

- On the **client**:

  - Folders like `components/` and `lib/` may exist both:
    - At `src` root level (shared across features)
    - Inside specific feature folders like `concepts/` (feature-scoped components)

- On the **server**:
  - The `middleware/` folder may appear:
    - At `src` root level for global middleware
    - Inside feature folders (e.g. `concepts/`) for domain-specific logic

### Validation âš”ï¸

Every form in the app is validated on **both the client and the server** using the same `Zod` schemas â€” shared via the `@shared/first` package.

- On the **client**, schemas validate user input immediately to improve UX.
- On the **server**, those same schemas are typically **extended** to handle backend-specific logic.

Special case: **file validation**

- On the **server**, files are **parsed** â€” which gives them a **different shape** than the browser's `File` object.
- As a result, the server uses a **custom Zod extension** to validate the parsed file structure correctly.

### Model Validations ğŸ“

With **Sequelize**, you can inject custom `JavaScript` logic directly into your model definitions â€” for example, validating fields dynamically during creation. This allows you to define a field as a generic `STRING` and rely on pre-create validation to enforce correctness.

In contrast, the **domain-specific language (DSL)** of **Prisma** doesn't allow embedding JavaScript logic inside the schema so I had to be stricter on model definition

- Fields that define a `category` or even `sub category` are defined using strict `enum` types.
- This guarantees type safety at the database level and prevents accidental insertion of invalid values

### Assets ğŸ“·

Assets stored in the cloud follow a **polymorphic model** â€” meaning there's a single `CloudAsset` entity instead of creating separate asset tables for each model (like `CourseImage` or `ConceptVideo`).

`Prisma` does not support polymorphic relationships as `Sequelize` so you will need to join assets manually with a custom aggregation in a `raw query`

To organize asset metadata and support flexible query, the `CloudAsset` model includes:

- **`entityType`** â€” Indicates whether the asset belongs to a `COURSE` or a `CONCEPT`
- **`type`** â€” Specifies the asset type, such as `IMAGE` or `VIDEO`
- **`entityId`** â€” The UUID of the associated record
- **`publicID`** â€” so you will be able also to delete assets from cloud if you want to implement a `PUT` or `DELETE` route ofr `courses` or `concepts`
- **`url`** â€” url returned by `cloudinary` on `upload`

### Courses ğŸ“š

The entire app flow starts with the **creation of a course**.

As mentioned in the `Model Validations ğŸ“` section, I used **Prisma `enum`s** to enforce stricter model constraints. These enums are autogenerated for the **server** in JavaScript, but since the **client** runs in its own isolated environment, it canâ€™t import anything directly from the server.

To keep types in sync, I manually recreated the enums inside the `@shared/first` package â€” making them available to both client and server.

Inside `@shared/first`, the object that stores the available `Tech` choices includes `stack` and `label` keys, but also a **`rootLanguage`** boolean.  
This value is auto-filled in the form when a tech is selected â€” but of course, you can override it if you think differently ğŸ˜ˆ

## Concepts ğŸ“š

A single `Course` could have many `concepts`.
In `Course` page to add a `Concept` is enough to click the `add concept` in `actions dropdown`.

For `Concept` as well as for `Course` you must add a `markdown` and some `images`, optional a video based on feelings

There are some fields with information that could be useful:

- **estimatedTime** â€” how much last until now the course
- **pointsGained** â€” how many points is possible to gain until now with course
- **order** â€” index of current concept your are adding

I established a generic range of `3 - 10` for questions you can add for a concept but you can easily modify the range in `@shared/first`

Each question includes:

- A **title** (to give the question some context)
- The **question** text itself
- Exactly **5 variants**, with only **1 correct** answer

In the database:

- Each question is linked to a `Concept`
- Each answer (variant) is linked to a specific `Question`

This structure results in:

- A `Concept` having an **array of questions**
- Each `Question` having an **array of variants**

## Quiz ğŸ˜«

The user can consolidate their knowledge at the end of the `Concept` page â€” after reading the `markdown` and watching if present the optional `video` â€” by completing a `quiz` containing all the questions defined for that specific concept.

After submit and successful validation will be created a `UserConcept`, I made it with thoughts abut a real environment with users so each `Concept` have many `UserConcept` that is possible to join via `conceptID` key constraint.

Each `UserConcept` includes:

- A **score**, calculated based on the `pointsGained` you defined for the related `Concept`
- An array of **UserAnswer** entries

Each `UserAnswer`:

- Includes a reference to the original `Variant` that was selected
- Has an `isCorrect` boolean flag so correctness can be checked without requiring a `join`
- Contains a reference to the related `Question` for easier aggregation

Then beside creating the `UserConcept` I marked the `Concept` as `completed` but is a step you could skip with real users, just the existence of `UserConcept` is enough to know the `Concept` has been completed

As well, is also done a check to know how many `concepts` of `Course` a user did and if all others `concepts` has been completed excluding current one then also `Course` will be marked as `completed`, but here also probably in a environment with `users` would be much better create a `UserCourse` table

### ğŸ–ï¸ UX

Updates related to **course** or **concept completion** are immediately reflected in the UI.

This is achieved by **invalidating the Redux Toolkit Query (RTK Query) cache**, which ensures the frontend always stays in sync with the most up-to-date server data.

## Final notes ğŸ§

I hope you find project interesting, if not the app does not have a refund policy ğŸ’°
Any kind of advice or suggestion is welcome to improve some parts of app or just my coding approach in general
